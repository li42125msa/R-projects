glm(formula = weight ~ Diet, data = ChickWeightEnd, family = 'gaussian')
model <- train(
quality ~ .,
tuneLength = 3,
tuneGrid=tuneGrid,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
plot(model)
varImp(model)
# Train a Random Forest
library(randomForest)
library(rpart)
library(caret)
library(e1071)
library(ipred)
library(caret)
library(Metrics)
library(readr)
library(gbm)
library(dplyr)
library(randomForest)
library(ROCR)
# Define the tuning grid: tuneGrid
tuneGrid <- data.frame(
.mtry = c(2, 3, 7),
.splitrule = "variance",
.min.node.size = 5
)
model <- train(
quality ~ .,
tuneLength = 3,
tuneGrid=tuneGrid,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
plot(model)
varImp(model)
plot(model)
plot(model)
# Train a Random Forest
library(randomForest)
library(rpart)
library(caret)
library(e1071)
library(ipred)
library(caret)
library(Metrics)
library(readr)
library(gbm)
library(dplyr)
library(randomForest)
library(ROCR)
?randomForest
set.seed(1)  # for reproducibility
# credit_train_r=credit_train
#
# library(dplyr)
# credit_train_r=credit_train %>% mutate_if(is.character, as.factor)
library(randomForest)
credit_model_randomForest <- randomForest(formula = default ~ .,
data = credit_train)
# Print the model output
print(credit_model_randomForest)
names(credit_model_randomForest)
head(credit_model_randomForest$predicted)
head(credit_model_randomForest$importance)
credit_model_randomForest$classes
# Generate predicted classes using the model object
# Grab OOB error matrix & take a look
err <- credit_model_randomForest$err.rate
head(err)
tail(err)
# Look at final OOB error rate (last row in err matrix)
oob_err <- err[nrow(err), "OOB"]
print(oob_err)
# Plot the model trained in the previous exercise
plot(credit_model_randomForest)
#
# library(dplyr)
# credit_train_r=credit_train %>% mutate_if(is.character, as.factor)
library(randomForest)
credit_model_randomForest <- randomForest(formula = default ~ .,
data = credit_train)
# Print the model output
print(credit_model_randomForest)
names(credit_model_randomForest)
head(credit_model_randomForest$predicted)
head(credit_model_randomForest$importance)
credit_model_randomForest$classes
# Generate predicted classes using the model object
# Grab OOB error matrix & take a look
err <- credit_model_randomForest$err.rate
head(err)
tail(err)
# Look at final OOB error rate (last row in err matrix)
oob_err <- err[nrow(err), "OOB"]
print(oob_err)
# Plot the model trained in the previous exercise
plot(credit_model_randomForest)
# Add a legend since it doesn't have one by default
legend(x = "bottom",
legend = colnames(err),
fill = 1:ncol(err))
# Generate predicted classes using the model object
pred_randomForest <- predict(object = credit_model_randomForest,  # model object
newdata = credit_test,  # test dataset
type = "class")         # return classification labels
# Calculate the confusion matrix for the test set
cm <- confusionMatrix(data = pred_randomForest,          # predicted classes
reference = credit_test$default)  # actual classes
print(cm)
# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
# Generate predictions on the test set
pred_randomForest1 <- predict(object = credit_model_randomForest,
newdata = credit_test,
type = "prob")
# `pred` is a matrix
class(pred)
# Look at the pred format
head(pred)
# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc_randomForest=auc(actual = ifelse(credit_test$default == "yes", 1, 0),
predicted = pred_randomForest1[,"yes"])
auc_randomForest
# Execute the tuning process
set.seed(1)
res <- tuneRF(x = subset(credit_train, select = -default),
y = credit_train$default,
ntreeTry = 500)
print(res)
# Establish a list of possible values for mtry, nodesize and sampsize
# Look at results
print(res)
# Find the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)
# Find the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)
mtry <- seq(4, ncol(credit_train) * 0.8, 2)
mtry
nodesize <- seq(3, 8, 2)
sampsize <- nrow(credit_train) * c(0.7, 0.8)
# Create a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
head(hyper_grid)
# Create an empty vector to store OOB error values
oob_err <- c()
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- randomForest(formula = default ~ .,
data = credit_train,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
sampsize = hyper_grid$sampsize[i])
# Store OOB error for the model
oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
opt_i
min(oob_err)
print(hyper_grid[opt_i,])
mtry <- seq(4, ncol(credit_train) * 0.8, 2)
mtry
nodesize <- seq(3, 8, 2)
sampsize <- nrow(credit_train) * c(0.7, 0.8)
# Create a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
head(hyper_grid)
# Create an empty vector to store OOB error values
oob_err <- c()
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- randomForest(formula = default ~ .,
data = credit_train,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
sampsize = hyper_grid$sampsize[i])
# Store OOB error for the model
oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
opt_i
min(oob_err)
print(hyper_grid[opt_i,])
tuneGrid <- data.frame(
.mtry = c(2, 3, 7),
.splitrule = "variance",
.min.node.size = 5
)
model <- train(
quality ~ .,
tuneLength = 3,
tuneGrid=tuneGrid,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
wine <- readRDS("C:/R/Project/Model/GBM/wine_100.RDS")
tuneGrid <- data.frame(
.mtry = c(2, 3, 7),
.splitrule = "variance",
.min.node.size = 5
)
model <- train(
quality ~ .,
tuneLength = 3,
tuneGrid=tuneGrid,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
plot(model)
varImp(model)
varImp(model)
plot(model)
install.packages("tictoc")
# Define the tuning grid: tuneGrid
tuneGrid <- data.frame(
.mtry = c(2, 3, 7),
.splitrule = "variance",
.min.node.size = 5
)
model <- train(
quality ~ .,
tuneLength = 3,
tuneGrid=tuneGrid,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
plot(model)
varImp(model)
?varImp
caret::varImp(model)
caret::varImp(model)
plot(model)
caret::varImp(model)
?varImp
set.seed(1)  # for reproducibility
# credit_train_r=credit_train
#
# library(dplyr)
# credit_train_r=credit_train %>% mutate_if(is.character, as.factor)
library(randomForest)
credit_model_randomForest <- randomForest(formula = default ~ .,
data = credit_train)
varImp(credit_model_randomForest)
model <- train(
quality ~ .,
tuneLength = 1,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
# Print model to console
model
varImp(model)
model <- train(
quality ~ .,
tuneLength = 1,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE,
importance = TRUE
)
)
# Print model to console
model
varImp(model)
varImp(model$finalModel))
varImp(model$finalModel)
varImp(model$finalModel)
ls(model)
model$finalModel
model <- train(
quality ~ .,
tuneLength = 1,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE,
importance = 'impurity'
)
)
# Print model to console
model
ls(model)
varImp(model$finalModel)
model <- train(
quality ~ .,
tuneLength = 1,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE,
importance = 'permutation'
)
)
# Print model to console
model
ls(model)
varImp(model$finalModel)
varImp(model)
model <- train(
quality ~ .,
tuneLength = 1,
data = wine,
method = "ranger",
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE,
,importance = 'impurity'
)
)
model <- train(
quality ~ .,
tuneLength = 1,
data = wine,
method = "ranger",
,importance = 'impurity'
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
model <- train(
quality ~ .,
tuneLength = 1,
data = wine,
method = "ranger",
,importance = 'impurity',
trControl = trainControl(
method = "cv",
number = 5,
verboseIter = TRUE
)
)
varImp(model)
model$finalModel
library(tidyverse)
gap_nested <- gapminder %>%
group_by(country) %>%
nest()
# Explore gap_nested
head(gap_nested)
library(tidyverse);library(gapminder)
gap_nested <- gapminder %>%
group_by(country) %>%
nest()
# Explore gap_nested
head(gap_nested)
gap_unnested <- gap_nested %>%
unnest()
# Confirm that your data was not modified
identical(gapminder, gap_unnested)
# Extract the data of Algeria
algeria_df <- gap_nested$data[[1]]
# Calculate the minimum of the population vector
min(algeria_df$population)
# Calculate the maximum of the population vector
max(algeria_df$population)
# Calculate the mean of the population vector
mean(algeria_df$population)
# Calculate the minimum of the population vector
min(algeria_df$population,na.rm = T)
# Extract the data of Algeria
algeria_df <- gap_nested$data[[1]]
# Calculate the minimum of the population vector
min(algeria_df$population,na.rm = T)
gap_nested$data[[1]]
gap_nested <- gapminder %>%
group_by(country) %>%
nest()
# Explore gap_nested
head(gap_nested)
gap_nested$data[[1]]
library(tidyverse);library(dslabs)
gap_nested <- gapminder %>%
group_by(country) %>%
nest()
# Explore gap_nested
head(gap_nested)
# Create the unnested dataframe called gap_unnnested
gap_unnested <- gap_nested %>%
unnest()
# Confirm that your data was not modified
identical(gapminder, gap_unnested)
# Extract the data of Algeria
algeria_df <- gap_nested$data[[1]]
# Calculate the minimum of the population vector
min(algeria_df$population,na.rm = T)
gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population))) %>%
unnest()
gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population)))
gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population))) %>%
unnest()
gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population))) %>%
unnest(pop_mean)
gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population)))
pop_df <- gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population)))
pop_df %>%
unnest(pop_mean)
pop_df <- gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population,na.rm = T)))
pop_df %>%
unnest(pop_mean)
pop_df
pop_df <- gap_nested %>%
mutate(pop_mean = map(data, ~mean(.x$population,na.rm = T))) %>%
unnest(pop_mean)
pop_df
library(tidyverse);library(dslabs)
gap_nested <- gapminder %>% na.omit()
group_by(country) %>%
nest()
# Explore gap_nested
head(gap_nested)
gapminder
gapminder %>% na.omit()
group_by(country) %>%
nest()
gapminder %>% na.omit()
gapminder %>% na.omit()
group_by(country)
names(gapminder)
gap_nested <- gapminder %>% na.omit()
group_by(country) %>%
nest()
gapminder
group_by(country) %>%
nest()
gapminder$country
gapminder
group_by(country)
library(tidyverse);library(dslabs)
names(gapminder)
gap_nested <- gapminder
group_by(country) %>%
nest()
gapminder
group_by(country)
# Prepare the nested dataframe gap_nested
library(tidyverse);library(gapminder)
names(gapminder)
gapminder
group_by(country)
head(gapminder)
gapminder
group_by(country)
library(caret)
# Fit lm model using 10-fold CV: model
model_lm <- train(
price ~ .,
diamonds,
method = "lm",
trControl = trainControl(
method = "cv",
number = 10,
verboseIter = TRUE
)
)
# Print model to console
model
