args(sample)
args(sample)
triple <- function(x) {
x <- 3*x
x
}
a <- 5
triple(a)
a
today <- Sys.Date()
format(Sys.Date(), format = "Today is a %A!")
# Load the gapminder package
library(gapminder)
# Load the dplyr package
library(dplyr)
# Look at the gapminder dataset
gapminder
data.frame(x =c(rep(1:10)), y=letters[1:10])
samples$t1 = ifelse(samples$x>6,2,1)
samples = data.frame(x =c(rep(1:10)), y=letters[1:10])
samples$t1 = ifelse(samples$x>6,2,1)
(samples$t1 = ifelse(samples$x>6,2,1))
samples
install.packages("skimr")
library(readr)
bakeoff <- read_csv("bakeoff.csv")
View(bakeoff)
library(readr)
desserts <- read_csv("desserts.csv")
View(desserts)
library(readr)
desserts_tidy <- read_csv("desserts_tidy.csv")
View(desserts_tidy)
library(readr)
messy_ratings <- read_csv("messy_ratings.csv")
View(messy_ratings)
library(readr)
messy_ratings2 <- read_csv("messy_ratings2.csv")
View(messy_ratings2)
bakers %>%
group_by(series) %>%
skim() # no argument needed here
bakeoff %>%
group_by(series) %>%
skim() # no argument needed here
View(bakeoff)
View(bakeoff)
library(skimr)
bakeoff %>%
group_by(series) %>%
skim() # no argument needed here
library(skimr)
library(tidyverse)
bakeoff %>%
group_by(series) %>%
skim() # no argument needed here
bakeoff %>%
group_by(series) %>%
skim() # no argument needed here
bakeoff %>%
group_by(series) %>%
skim() %>%
summary()# no argument needed here
baker %>%
distinct(result)
bakeoff %>%
distinct(result)
bakeoff %>%
count(aired_us, series)
View(bakeoff)
# Find format to parse uk_airdate
parse_date("17 August 2010", format = "%d %B %Y")
library(readr)
bakeoff <- read_csv("bakeoff.csv")
View(bakeoff)
ggplot(bakeoff, aes(episode)) +
geom_bar() +
facet_wrap(~series)
ggplot(bakeoff, aes(episode)) +
geom_bar() +
facet_grid(~series)
# Try to cast technical as a number
desserts <- read_csv("desserts.csv",
col_types = cols(
technical = col_number()
))
# Try to cast technical as a number
# Cast result a factor
library(readr)
desserts <- read_csv("desserts.csv",
na = c("", "NA", "N/A"),
col_types = cols(
technical = col_number(),
uk_airdate = col_date(format = "%d %B %Y"),
result = col_factor(levels = NULL)
))
# Glimpse to view
glimpse(desserts)
install.packages("janitor")
# Plot of episode 1 viewers by series
ggplot(messy_ratings, aes(x = series, y = e1)) +
geom_col()
library(ggplot2)
ggplot(messy_ratings, aes(x = series, y = e1)) +
geom_col()
View(messy_ratings)
tidy_ratings <- messy_ratings %>%
# Gather and convert episode to factor
gather(key = "episode", value = "viewers_7day", -series,
factor_key = TRUE, na.rm = TRUE)
tidy_ratings <- messy_ratings %>%
# Gather and convert episode to factor
gather(key = "episode", value = "viewers_7day", -series,
factor_key = TRUE, na.rm = TRUE)
messy_ratings %>%
# Gather and convert episode to factor
gather(key = "episode", value = "viewers_7day", -series,
factor_key = TRUE, na.rm = TRUE)
tidy_ratings <- messy_ratings %>%
# Gather and convert episode to factor
gather(key = "episode", value = "viewers_7day", -series, factor_key = TRUE, na.rm = TRUE)
# Plot of episode 1 viewers by series
library(ggplot2);library(dplyr)
tidy_ratings <- messy_ratings %>%
# Gather and convert episode to factor
gather(key = "episode", value = "viewers_7day", -series, factor_key = TRUE, na.rm = TRUE)
# Plot of episode 1 viewers by series
library(ggplot2);library(dplyr);library(tidyverse)
tidy_ratings <- messy_ratings %>%
# Gather and convert episode to factor
gather(key = "episode", value = "viewers_7day", -series, factor_key = TRUE, na.rm = TRUE)
View(tidy_ratings)
View(messy_ratings2)
install.packages("forcats")
library(readr)
library(skimr)
library(tidyverse)
bakeoff %>%
distinct(result)
library(readr)
bakers
bakeoff
library(skimr)
glimpse(bakeoff)
skim(bakeoff)
bakeoff%>%
skim() %>%  # no argument needed here
summary() # no argument needed here
bakeoff %>%
count(aired_us, series) %>%
bakeoff %>%
count(aired_us, series)
bakers %>%
count(us_season,result)
bakeoff %>%
count(us_season,result)
bakeoff %>%
count(us_season,result) %>%
mutate(prop_bakers = n/sum(n))
bakeoff %>%
group_by(us_season,result) %>%
summarize(n = n())
bakeoff %>%
group_by(us_season,result) %>%
summarize(n = n()) %>%
ungroup() %>%
mutate(prop_bakers = n/sum(n))
bakeoff %>%
count(us_season,result) %>%
count(us_season)
# View distinct results
bakeoff %>%
distinct(result)
desserts %>%
count(nut, sort = TRUE)
ggplot(bakeoff, aes(episode)) +
geom_bar() +
facet_grid(~series)
ggplot(bakeoff, aes(episode)) +
geom_bar() +
facet_wrap(~series)
library(readr)
desserts <- read_csv("desserts.csv",
na = c("", "NA", "N/A"),
col_types = cols(
technical = col_number(),
uk_airdate = col_date(format = "%d %B %Y"),
result = col_factor(levels = NULL)
))
# Glimpse to view
glimpse(desserts)
# Count rows grouping by nut variable
desserts %>%
count(nut, sort = TRUE)
library(readr)
desserts <- read_csv("desserts.csv",
na = c("", "NA", "N/A"),
col_types = cols(
technical = col_number(),
uk_airdate = col_date(format = "%d %B %Y"),
result = col_factor(levels = NULL)
))
# Count rows grouping by nut variable
desserts %>%
count(nut, sort = TRUE)
# Glimpse to view
glimpse(desserts)
ratings %>%
select(channel, everything())
# Select & change variable names
young_bakers3 %>%
select(baker, tech_ = tre1:tre3)
# Reorder from most to least bakers
library(forcats)
ggplot(bakers, aes(x = fct_infreq(gen))) +
geom_bar()
# Reorder from most to least bakers
library(forcats);library(ggplot2)
ggplot(bakers, aes(x = fct_infreq(gen))) +
geom_bar()
View(bakeoff)
ggplot(bakeoff, aes(x = fct_infreq(techinical))) +
geom_bar()
View(bakeoff)
View(bakeoff)
ggplot(bakeoff, aes(x = fct_infreq(technical))) +
geom_bar()
ggplot(bakeoff, aes(x = fct_infreq(factor(technical))) +
geom_bar()
ggplot(bakeoff, aes(x = fct_infreq(factor(technical))) +
geom_bar()
ggplot(bakeoff, aes(x = fct_infreq(factor(technical)))) +
geom_bar()
library(lubridate)
dmy("17 August 2010")
tibble::tribble(
~host, ~bday, ~premiere,
"Mary", "24 March 1935", "August 17th, 2010",
"Paul", "1 March 1966", "August 17th, 2010")
hosts %>%
mutate(bday = dmy(bday),
premiere = mdy(premiere))
hosts <- tibble::tribble(
~host, ~bday, ~premiere,
"Mary", "24 March 1935", "August 17th, 2010",
"Paul", "1 March 1966", "August 17th, 2010")
hosts <- hosts %>%
mutate(bday = dmy(bday),
premiere = mdy(premiere))
library(lubridate);library(dplyr)
hosts <- tibble::tribble(
~host, ~bday, ~premiere,
"Mary", "24 March 1935", "August 17th, 2010",
"Paul", "1 March 1966", "August 17th, 2010")
hosts <- hosts %>%
mutate(bday = dmy(bday),
premiere = mdy(premiere))
hosts %>%
mutate(bday = dmy(bday),
premiere = mdy(premiere))
tibble::tribble(
~host, ~bday, ~premiere,
"Mary", "24 March 1935", "August 17th, 2010",
"Paul", "1 March 1966", "August 17th, 2010")
hosts <- tibble::tribble(
~host, ~bday, ~premiere,
"Mary", "24 March 1935", "August 17th, 2010",
"Paul", "1 March 1966", "August 17th, 2010")
hosts %>%
mutate(bday = dmy(bday),
premiere = mdy(premiere))
year(1)
year(1)
library(lubridate);library(dplyr)
year(1)
years(1)
years(2)
months(12)
years(1)-months(12)
years(1)- months(12)
library(readr);library(stringr)
install.packages("here")
library(here)
here()
install.packages("GGMAP")
install.packages("ggmap")
source('~/.active-rstudio-document', echo=TRUE)
library(readr)
smc_with_js <- read_csv("smc_with_js.csv")
View(smc_with_js)
library(readr)
multiple_choice_responses<- read_csv("smc_with_js.csv")
is.factor(multiple_choice_responses$CurrentJobTitleSelect)
responses_as_factors %>%
# apply the function nlevels to each column
summarise_all(nlevels)
library(tidyverse)
# Change all the character columns to factors
responses_as_factors <- multiple_choice_responses %>%
mutate_if(is.character, as.factor)
responses_as_factors %>%
# apply the function nlevels to each column
summarise_all(nlevels)
View(responses_as_factors)
View(responses_as_factors)
responses_as_factors %>%
# apply the function nlevels to each column
summarise_all(nlevels) %>%
# change the dataset from wide to long
gather(variable, num_levels)
number_of_levels %>%
# filter for where variable is CurrentJobTitleSelect
filter(variable == "CurrentJobTitleSelect") %>%
# pull num_levels
pull(num_levels)
number_of_levels <- responses_as_factors %>%
# apply the function nlevels to each column
summarise_all(nlevels) %>%
# change the dataset from wide to long
gather(variable, num_levels)
number_of_levels %>%
# filter for where variable is CurrentJobTitleSelect
filter(variable == "CurrentJobTitleSelect") %>%
# pull num_levels
pull(num_levels)
library(forcats)
# Make a bar plot
ggplot(multiple_choice_responses, aes(x = EmployerIndustry)) +
geom_bar() +
# flip the coordinates
coord_flip()
names(multiple_choice_responses)
# Make a bar plot
ggplot(multiple_choice_responses, aes(x = CurrentJobTitleSelect)) +
geom_bar() +
# flip the coordinates
coord_flip()
ggplot(multiple_choice_responses, aes(x = fct_infreq(CurrentJobTitleSelect)) +
geom_bar() +
# flip the coordinates
coord_flip()
# Make a bar plot
ggplot(multiple_choice_responses, aes(x = fct_infreq(CurrentJobTitleSelect))) +
geom_bar() +
# flip the coordinates
coord_flip()
ggplot(multiple_choice_responses, aes(x = fct_infreq(CurrentJobTitleSelect))) +
geom_bar() +
# flip the coordinates
coord_flip()
ggplot(aes(nlp_frequency,
x = fct_relevel(response,
"Rarely", "Sometimes", "Often", "Most of the time"))) +
geom_bar()
f <- function(x) {
if (TRUE) {
return(x + 1)
}
x
}
f(2)
y <- 10
f <- function(x) {
x + y
}
f(10)
typeof(x)
typeof(y)
class(y)
cyl<- split(mtcars,mtcars$cyl)
str(cyl)
cyl[[1]]
models <- map(cyl, ~ lm(mpg ~ wt, data = .))
# Save the result from the previous exercise to the variable models
models <- purrr::map(cyl, ~ lm(mpg ~ wt, data = .))
# Use map and coef to get the coefficients for each model: coefs
coefs <- map(models, coef)
library(purrr)
models
# Use map and coef to get the coefficients for each model: coefs
coefs <- map(models, coef)
map(models, coef)
map(coefs, "wt")
# Define models (don't change)
models <- mtcars %>%
split(mtcars$cyl) %>%
map(~ lm(mpg ~ wt, data = .))
models %>%
map(summary)
models %>%
map(summary) %>%
map_dbl("r.squared")
# Define list of functions
funs <- list(Normal = "rnorm", Uniform = "runif", Exp = "rexp")
# Define params
params <- list(
Normal = list(mean = 10),
Uniform = list(min = 0, max = 5),
Exp = list(rate = 5)
)
# Assign the simulated samples to sims
sims <- invoke_map(funs, params, n = 5000)
sims
# Use walk() to make a histogram of each element in sims
walk(sims, hist)
# Create x, a vector, with values NA, NaN, Inf, ".", and "missing"
x <- c(NA, NaN, Inf, ".", "missing")
# Use any_na() and are_na() on to explore the missings
any_na(x)
are_na(x)
# Create x, a vector, with values NA, NaN, Inf, ".", and "missing"
x <- c(NA, NaN, Inf, ".", "missing")
# Use any_na() and are_na() on to explore the missings
any_na(x)
?any_na
anyNA(x)
areNA(x)
?are_na
are_na(x)
install.packages("rlang")
library(rlang)
install.packages("rlang")
install.packages("rlang")
library("rlang", lib.loc="~/R/R-3.5.3/library")
# Use any_na() and are_na() on to explore the missings
any_na(x)
are_na(x)
?any_na
# Use any_na() and are_na() on to explore the missings
anyNA(x)
are_na(x)
# Use n_miss() to count the total number of missing values in dat_hw
n_miss(x)
?n_missing
?n_miss
?n_miss
??n_miss
# Use n_miss() to count the total number of missing values in dat_hw
n_miss(x)
install.packages("naniar")
library(rlang);library(naniar)
# Use n_miss() to count the total number of missing values in dat_hw
n_miss(x)
# Use n_complete() on dat_hw to count the total number of complete values
n_complete(x)
View(bakeoff)
View(bakeoff)
View(df)
View(df)
View(hosts)
View(mydata10)
View(mydata10)
View(mydata2)
View(mydata2)
# Summarise missingness in each variable of the `airquality` dataset
miss_var_summary(airquality)
# Tabulate missingness in each variable and case of the `airquality` dataset
miss_var_table(airquality)
miss_case_table(airquality)
airquality
# Tabulate missingness in each variable and case of the `airquality` dataset
miss_var_table(airquality)
# Summarise missingness in each variable of the `airquality` dataset
miss_var_summary(airquality)
miss_case_table(airquality)
airquality %>% group_by(Month)
library(dplyr)
airquality %>% group_by(Month)
airquality %>% group_by(Month) %>% miss_var_table()
# Tabulate of missingness in each case, grouped by Month, in the `airquality` dataset
airquality %>% group_by(Month) %>% miss_case_table()
# Visualize all of the missingness in the `riskfactors`  dataset
vis_miss(riskfactors)
riskfactors
# Visualize and cluster all of the missingness in the `riskfactors` dataset
vis_miss(riskfactors, cluster = TRUE)
# Visualize and sort the columns by missingness in the `riskfactors` dataset
vis_miss(riskfactors, sort_miss = TRUE)
# Visualize and sort the columns by missingness in the `riskfactors` dataset
vis_miss(riskfactors, sort_miss = TRUE)
# Visualize the number of missings in cases using `gg_miss_case()`
gg_miss_case(riskfactors)
# Explore the number of missings in cases using `gg_miss_case()` and facet by the variable `education`
gg_miss_case(riskfactors, facet = education)
# Visualize the number of missings in variables using `gg_miss_var()`
gg_miss_var(riskfactors)
# Explore the number of missings in variables using `gg_miss_var()` and facet by the variable `education`
gg_miss_var(riskfactors, facet = education)
# Using the airquality dataset, explore the missingness pattern using gg_miss_upset()
gg_miss_upset(airquality)
# With the riskfactors dataset, explore how the missingness changes across the marital variable using gg_miss_fct()
gg_miss_fct(x = riskfactors, fct = marital)
# Using the pedestrian dataset, explore how the missingness of hourly_counts changes over a span of 3000
gg_miss_span(pedestrian, var = hourly_counts, span_every = 3000)
# Using the pedestrian dataset, explore the impact of month by facetting by month
# and explore how missingness changes for a span of 1000
gg_miss_span(pedestrian, var = hourly_counts, span_every = 1000, facet = month)
# Using the pedestrian dataset, explore the impact of month by facetting by month
# and explore how missingness changes for a span of 1000
gg_miss_span(pedestrian, var = hourly_counts, span_every = 1000, facet = month)
# Explore the strange missing values "N/A"
miss_scan_count(data = pacman, search = list("N/A"))
frogger
library(tidyr)
grepl("edu", emails)
# The emails vector has already been defined for you
emails <- c("john.doe@ivyleague.edu", "education@world.gov", "dalai.lama@peace.org",
"invalid.edu", "quant@bigdatacollege.edu", "cookie.monster@sesame.tv")
# Use grepl() to match for "edu"
grepl("edu", emails)
cyl<- split(mtcars,mtcars$cyl)
str(cyl)
